{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\abcd_v1.1.json created from C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\abcd_v1.1.json.gz\n",
      "Data in C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\abcd_sample.json has inconsistent lengths: [5, 29, 29, 5, 21, 21, 5, 22, 22]\n",
      "Data in C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\abcd_v1.1.json has inconsistent lengths: [8034, 1004, 1004]\n",
      "Loaded data from C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\guidelines.json\n",
      "Column names in C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\guidelines.json: ['Product Defect', 'Order Issue', 'Account Access', 'Troubleshoot Site', 'Manage Account', 'Purchase Dispute', 'Shipping Issue', 'Subscription Inquiry', 'Single-Item Query', 'Storewide Query']\n",
      "First few rows of C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\guidelines.json:\n",
      "                                                Product Defect  \\\n",
      "description                                refunds and returns   \n",
      "subflows     {'Initiate Refund': {'actions': [{'type': 'int...   \n",
      "\n",
      "                                                   Order Issue  \\\n",
      "description  get status of an order or change an order, pos...   \n",
      "subflows     {'Status Mystery Fee': {'actions': [{'type': '...   \n",
      "\n",
      "                                                Account Access  \\\n",
      "description  username, password, and two-factor authentication   \n",
      "subflows     {'Recover Username': {'actions': [{'type': 'in...   \n",
      "\n",
      "                                             Troubleshoot Site  \\\n",
      "description  website slow, search not working, credit card,...   \n",
      "subflows     {'Invalid Credit Card': {'actions': [{'type': ...   \n",
      "\n",
      "                                                Manage Account  \\\n",
      "description  subscription service added or removed, change ...   \n",
      "subflows     {'Status Service Added': {'actions': [{'type':...   \n",
      "\n",
      "                                              Purchase Dispute  \\\n",
      "description      bad price, out of stock, promo codes, billing   \n",
      "subflows     {'Bad Price Competitor': {'actions': [{'type':...   \n",
      "\n",
      "                                                Shipping Issue  \\\n",
      "description             check our update a shipment of an item   \n",
      "subflows     {'Shipping Status': {'actions': [{'type': 'int...   \n",
      "\n",
      "                                          Subscription Inquiry  \\\n",
      "description  billing and updates of the premium subscriptio...   \n",
      "subflows     {'Status Active': {'actions': [{'type': 'inter...   \n",
      "\n",
      "                                             Single-Item Query  \\\n",
      "description  FAQ questions about jeans, boots, shirt or swe...   \n",
      "subflows     {'Boots FAQ': {'actions': [{'type': 'faq/polic...   \n",
      "\n",
      "                                               Storewide Query  \n",
      "description  FAQ questions about pricing, timing, membershi...  \n",
      "subflows     {'Pricing FAQ': {'actions': [{'type': 'faq/pol...  \n",
      "Data in C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\kb.json has inconsistent lengths: [2, 3, 3, 5, 4, 3, 4, 4, 4, 4, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 4, 4, 3, 4, 4, 4, 5, 5, 5, 4, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5]\n",
      "Data in C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\ontology.json has inconsistent lengths: [2, 2, 3, 2, 3]\n",
      "Data in C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\utterances.json has inconsistent lengths: []\n",
      "Combined DataFrame created successfully.\n",
      "Columns in combined_df:\n",
      "Index(['Product Defect', 'Order Issue', 'Account Access', 'Troubleshoot Site',\n",
      "       'Manage Account', 'Purchase Dispute', 'Shipping Issue',\n",
      "       'Subscription Inquiry', 'Single-Item Query', 'Storewide Query'],\n",
      "      dtype='object')\n",
      "Data preparation and cleaning complete. Training and testing data saved.\n",
      "Conversation Data: [{'user': 'Hi', 'assistant': 'Hello! How can I help you?'}, {'user': 'I need help with my code.', 'assistant': \"Sure, what's the issue?\"}]\n",
      "Structured Data: {'description': 'This is a structured data example.', 'steps': ['Step 1', 'Step 2', 'Step 3']}\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to unzip .gz file\n",
    "def unzip_gz_file(gz_filename, output_filename):\n",
    "    with gzip.open(gz_filename, 'rb') as file_in:\n",
    "        with open(output_filename, 'wb') as file_out:\n",
    "            shutil.copyfileobj(file_in, file_out)\n",
    "    print(f\"{output_filename} created from {gz_filename}\")\n",
    "\n",
    "# Function to load JSON files\n",
    "def load_json(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except ValueError as e:\n",
    "        print(f\"Error loading JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "# File paths (location)\n",
    "abcd_filename = r\"C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\abcd_sample.json\"\n",
    "gz_filename = r\"C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\abcd_v1.1.json.gz\"\n",
    "unzipped_filename = r\"C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\abcd_v1.1.json\"\n",
    "guidelines_filename = r\"C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\guidelines.json\"\n",
    "kb_filename = r\"C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\kb.json\"\n",
    "ontology_filename = r\"C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\ontology.json\"\n",
    "utterances_filename = r\"C:\\Users\\lucia\\OneDrive\\Desktop\\BTTAI codes\\ASAPP\\abcd-master\\abcd-master\\data\\utterances.json\"\n",
    "\n",
    "# Unzip the .gz file\n",
    "unzip_gz_file(gz_filename, unzipped_filename)\n",
    "\n",
    "# Loads data from all files\n",
    "data_files = [abcd_filename, unzipped_filename, guidelines_filename, kb_filename, ontology_filename, utterances_filename]\n",
    "data_frames = []\n",
    "\n",
    "for file in data_files:\n",
    "    if os.path.exists(file):\n",
    "        data = load_json(file)\n",
    "        if data:\n",
    "            if isinstance(data, dict):\n",
    "                lengths = [len(v) for v in data.values()]\n",
    "            elif isinstance(data, list):\n",
    "                lengths = [len(v) for item in data if isinstance(item, dict) for v in item.values() if isinstance(v, (list, dict, str))]\n",
    "            else:\n",
    "                print(f\"Unexpected data type: {type(data)}\")\n",
    "                continue\n",
    "\n",
    "            if len(set(lengths)) == 1:\n",
    "                df = pd.DataFrame(data)\n",
    "                data_frames.append(df)\n",
    "                print(f\"Loaded data from {file}\")\n",
    "                print(f\"Column names in {file}: {df.columns.tolist()}\")\n",
    "                print(f\"First few rows of {file}:\")\n",
    "                print(df.head())\n",
    "            else:\n",
    "                print(f\"Data in {file} has inconsistent lengths: {lengths}\")\n",
    "    else:\n",
    "        print(f\"File {file} does not exist\")\n",
    "\n",
    "# Combines all data frames into one\n",
    "if data_frames:\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    print(\"Combined DataFrame created successfully.\")\n",
    "else:\n",
    "    print(\"No data frames to combine.\")\n",
    "    combined_df = pd.DataFrame()  # Creates an empty DataFrame to avoid NameError\n",
    "\n",
    "# Inspects the DataFrame columns\n",
    "print(\"Columns in combined_df:\")\n",
    "print(combined_df.columns)\n",
    "\n",
    "# Data Cleaning\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # Converts to lowercase\n",
    "        text = re.sub(r'\\d+', '', text)  # Removes numbers\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Removes extra spaces\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Removes punctuation\n",
    "    elif isinstance(text, dict):\n",
    "        text = {k: clean_text(v) for k, v in text.items()}  # Recursively clean dictionary values\n",
    "    elif isinstance(text, list):\n",
    "        text = [clean_text(item) for item in text]  # Recursively clean list items\n",
    "    return text\n",
    "\n",
    "# List of column names to clean\n",
    "text_column_names = ['Product Defect', 'Order Issue', 'Account Access', 'Troubleshoot Site', 'Manage Account', 'Purchase Dispute', 'Shipping Issue', 'Subscription Inquiry', 'Single-Item Query', 'Storewide Query'] \n",
    "\n",
    "# Apply the clean_text function to each column in the list\n",
    "for column in text_column_names:\n",
    "    if column in combined_df.columns:\n",
    "        combined_df[f'cleaned_{column}'] = combined_df[column].apply(clean_text)\n",
    "    else:\n",
    "        print(f\"Column {column} does not exist in the DataFrame\")\n",
    "\n",
    "# Handles missing values\n",
    "for column in text_column_names:\n",
    "    cleaned_column = f'cleaned_{column}'\n",
    "    if cleaned_column in combined_df.columns:\n",
    "        combined_df.dropna(subset=[cleaned_column], inplace=True)\n",
    "\n",
    "# Tokenization\n",
    "for column in text_column_names:\n",
    "    cleaned_column = f'cleaned_{column}'\n",
    "    if cleaned_column in combined_df.columns:\n",
    "        combined_df[f'tokens_{column}'] = combined_df[cleaned_column].apply(lambda x: x.split() if isinstance(x, str) else x)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save cleaned and prepared data\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "\n",
    "print(\"Data preparation and cleaning complete. Training and testing data saved.\")\n",
    "\n",
    "# Example data structure\n",
    "data = {\n",
    "    \"original\": {\n",
    "        \"conversation\": [\n",
    "            {\"user\": \"Hi\", \"assistant\": \"Hello! How can I help you?\"},\n",
    "            {\"user\": \"I need help with my code.\", \"assistant\": \"Sure, what's the issue?\"}\n",
    "        ]\n",
    "    },\n",
    "    \"scenario\": {\n",
    "        \"description\": \"This is a structured data example.\",\n",
    "        \"steps\": [\"Step 1\", \"Step 2\", \"Step 3\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Accessing the conversation data\n",
    "conversation_data = data.get(\"original\", {}).get(\"conversation\", [])\n",
    "print(\"Conversation Data:\", conversation_data)\n",
    "\n",
    "# Accessing the structured data\n",
    "structured_data = data.get(\"scenario\", {})\n",
    "print(\"Structured Data:\", structured_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
